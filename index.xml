<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on ICASSP 2026: Human-like Spoken Dialogue Systems Challenge</title>
    <link>https://aslp-lab.github.io/HumDial-Challenge/</link>
    <description>Recent content in Home on ICASSP 2026: Human-like Spoken Dialogue Systems Challenge</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://aslp-lab.github.io/HumDial-Challenge/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>General description</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/homepage_sections/general_description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/homepage_sections/general_description/</guid>
      <description>News and Updates December 13, 2025: We are pleased to announce the challenge results: Track 1 Results, Track 2 Results
November 17, 2025: Upon review, anomalies were identified in a small number of dialogue data samples. To ensure the accuracy and reliability of the evaluation results, the following items will be excluded from the final evaluation:
Track 1: task2_4_en_0002_0014 and items with dialogue_id prefixes task2_3_zh_0015, task2_4_zh_0015, and task2_5_zh_0015. Track 2: test-003126.</description>
    </item>
    
    <item>
      <title>Get started</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/homepage_sections/leaderboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/homepage_sections/leaderboard/</guid>
      <description>Registration Teams can register by the google form: https://docs.google.com/forms/d/e/1FAIpQLSdRrlfqrhh8QhOxtKMr03AxnnX14md_EwFuIuMt-Hf4fhhARA/viewform?usp=header
Reminder! Please use your institutional or corporate email address to register, and avoid using personal email accounts.</description>
    </item>
    
    <item>
      <title>Timeline</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/homepage_sections/timeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/homepage_sections/timeline/</guid>
      <description>Timeline August 20, 2025: Registration opens September 29, 2025: Release of training set, validation set, and baseline system November 10, 2025: Release of test set November 25, 2025: Submission deadline December 7, 2025: Deadline for submitting 2-page papers to ICASSP 2026 (invited teams only) January 11, 2026: Notification of acceptance for 2-page ICASSP 2026 papers January 18, 2026: Submission of final version of papers May 4–8, 2026: ICASSP 2026 Conference, Barcelona, Spain Official Competition Rules 1.</description>
    </item>
    
    <item>
      <title>Organizers</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/homepage_sections/organizers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/homepage_sections/organizers/</guid>
      <description>Organizers The challenge is organized by a distinguished team of researchers:
Lei Xie, Professor, Northwestern Polytechnical University Shuai Wang, Associate Professor, Nanjing University Haizhou Li, Professor, Chinese University of Hong Kong Eng Siong Chng, Professor, Nanyang Technological University Hung-yi Lee, Professor, Natioanl Taiwan University Chao Zhang, Assistant Professor, Tsinghua University Guangzhi Sun, Junior Research Fellow, University of Cambridge Xixin Wu, Assistant Professor, Chinese University of Hong Kong Longshuai Xiao, Huawei Technologies Zihan Zhang, Huawei Technologies Xinsheng Wang, Soul AI Lab Hui Bu, AISHELL Xin Xu， AISHELL Zhixian Zhao, Northwestern Polytechnical University Hongfei Xue, Northwestern Polytechnical University Xuelong Geng, Northwestern Polytechnical University GuoJian Li, Northwestern Polytechnical University Shuiyuan Wang, Northwestern Polytechnical University Contact For any inquiries, please contact:</description>
    </item>
    
    <item>
      <title>Results</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/track1/results/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/track1/results/</guid>
      <description>Emotional Intelligence Track Results 1. Task Dimensions &amp;amp; Evaluation Methodology The final score for this challenge track combines both automated metrics and human evaluation to ensure the results are professional and objective.
Evaluation Environment: The automated evaluation utilizes the Qwen/Qwen3-Omni-30B-A3B-Instruct model, which is deployed entirely in a local environment for scoring. For the detailed evaluation prompts, please refer to the competition guidelines.
Task 1: Emotional Trajectory Detection
Dimension 1: Accuracy_Completeness Dimension 2: Depth_Granularity Dimension 3: Added_Value Task 2: Emotional Reasoning</description>
    </item>
    
    <item>
      <title>Results</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/track2/results/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/track2/results/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Test Set</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/track1/test_set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/track1/test_set/</guid>
      <description>Test Set Description The test set (HD-Track1-Test) is now officially available, containing both Chinese (zh) and English (en) subsets. Each language subset provides 150 multi-turn dialogues for each of the three tasks (Task 1, 2 &amp;amp; 3). For each task, we provide a corresponding audio folder and a JSONL file. This file outlines the dialogue structure, such as turn order and audio file mapping.
HD-Track1-Test |_zh |__HD-Track1-T1/ |_{dialogue_id}_{turn_id}.wav |__HD-Track1-T2/ |__HD-Track1-T3/ |__HD-Track1-T1.</description>
    </item>
    
    <item>
      <title>Test Set Track 2</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/track2/test_set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/track2/test_set/</guid>
      <description>Test Set Description The test set (HD-Track2-Test) has been officially released and contains two subsets: one in Chinese and one in English. Each language subset covers two major scenarios—Interruption and Rejection. The dataset consists of 9,100 samples in total, including:
5,000 official test samples: test-{dialogue_id}.wav (Chinese/English ratio = 1:1) 4,100 clean audio files required for scoring interruption and rejection: clean-{dialogue_id}.wav Note: The data has been randomly shuffled. The test.wav and clean.</description>
    </item>
    
    <item>
      <title>Track 1: Emotional Intelligence</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/track1/description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/track1/description/</guid>
      <description>Challenge Tasks Task 1: Emotional Trajectory Detection - Evaluate the model&amp;rsquo;s ability to accurately identify and concisely summarize a user&amp;rsquo;s emotional changes throughout a multi-turn conversation. Task 2: Emotional Reasoning - Evaluate the model&amp;rsquo;s ability to perceive the underlying causes of a user&amp;rsquo;s emotions. Task 3: Empathy Assessment - Evaluate the model&amp;rsquo;s ability to generate empathetic responses in both text and audio formats. The final ranking will be determined based on the comprehensive score of the above three core tasks, and the specific weights of each task will be announced in subsequent stages.</description>
    </item>
    
    <item>
      <title>Track 2: Full-Duplex Interaction</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/track2/description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/track2/description/</guid>
      <description>Challenge Tasks The full-duplex benchmark primarily encompasses two major Scenarios: interruption and rejection.
1. Interruption Scenarios: Follow-up Questions: The user poses a follow-up question based on the model’s previous response, interrupting the ongoing output. The model should promptly address the user’s follow-up inquiry. Negation / Dissatisfaction: The user expresses dissatisfaction or disagreement with the model’s response using negative statements, interrupting the model mid-sentence. The model should appropriately react to the user’s negation or dissatisfaction in a timely manner.</description>
    </item>
    
    <item>
      <title>FAQ</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/faq/</guid>
      <description>General Information 1. Q: Unable to register using Google Form.
A: If registration via Google Form fails, users in mainland China can also sign up using the following link: [Tencent Docs] ICASSP 2026 HumDial Challenge Registration https://docs.qq.com/form/page/DY2tvT3FvWnRMZXRp
2. Q: Are there any restrictions on the type and size of base models that can be used for the competition?
A: There are no restrictions on the type or size of the models.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/registration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/registration/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
