<!DOCTYPE HTML>
<html lang="en">
    <head>

<title>General description | ICASSP 2026: Human-like Spoken Dialogue Systems Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="/HumDial-Challenge/style.css" />
<meta property="og:title" content="General description" />
<meta property="og:description" content="General description" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aslp-lab.github.io/HumDial-Challenge/homepage_sections/general_description/" /><meta property="article:section" content="homepage_sections" />



</head>
    <body class="is-preload">
        <div id="page-wrapper"><div id="header">
    <h1><a href="https://aslp-lab.github.io/HumDial-Challenge/" id="logo">
        ICASSP 2026: Human-like Spoken Dialogue Systems Challenge
    </a></h1>

    <nav id="nav">
        <ul>
                <li class="">
                    <a href="/HumDial-Challenge/">Home</a>
                <li class="">
                    <a href="#">Track 1: Emotional Intelligence</a><ul>
                        <li class="">
                            <a href="/HumDial-Challenge/task1/description">Description</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task1/test_set">Test_set</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task1/leaderboard">Leaderboard</a>
                    </ul>
                <li class="">
                    <a href="#">Track 2: Full-Duplex Interaction</a><ul>
                        <li class="">
                            <a href="/HumDial-Challenge/task2/description">Description</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task2/test_set">Test_set</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task2/leaderboard">Leaderboard</a>
                    </ul>
                <li class="">
                    <a href="/HumDial-Challenge/faq/">FAQ</a>
        </ul>
    </nav>
</div>

            <section class="wrapper style1">
                <div class="container">
                    

                        <div class="">
                            <div id="content">
                                <article>
    <header>
        <h2>General description</h2>
        <p>General description</p>
        
        
        <ul class="tags">
</ul>

    </header><h2 id="news-and-updates">News and Updates</h2>
<p><strong>November 17, 2025</strong>:Upon review, anomalies were identified in a small number of dialogue data samples. To ensure the accuracy and reliability of the evaluation results, the following items will be excluded from the final evaluation:</p>
<ul>
<li><strong>Track 1</strong>: task2_4_en_0002_0014, Items with dialogue_id prefixes task2_3_zh_0015, task2_4_zh_0015, and task2_5_zh_0015.</li>
<li><strong>Track 2</strong>: test-003126.wav、clean-000899.wav</li>
</ul>
<p><strong>November 17, 2025</strong>:The test set and submission rules are released. Please refer to the corresponding page of each track for specific information.</p>
<p><strong>November 11, 2025</strong>: Due to the delayed release of the training data, the test data will also be postponed. We expect to release the test set around November 17. Please stay tuned.</p>
<p><strong>October 29, 2025</strong>: We have released the baseline model, and you can obtain the model files from the two track pages.</p>
<p><strong>October 10, 2025</strong>: We have sent the training and dev dataset access instructions and other relevant information for this challenge to all successfully registered teams via email.</p>
<p>If you have not received the email, please:</p>
<ul>
<li>Check the email you used for registration: Please confirm that you used your institutional email instead of personal email (such as QQ, Gmail, 163, etc.) for registration. We only sent emails to valid institutional email addresses.</li>
<li>Check your email&rsquo;s spam or subscription folder.</li>
</ul>
<h2 id="challenge-call">Challenge Call</h2>
<p>Have you been following the recent buzz around the impressive performance of next-generation voice dialogue models like GPT-4o, Doubao, and the newly released GPT-Realtime? They are not only lightning-fast and expressive but also enable seamless multimodal interactions, making conversations feel remarkably human.</p>
<p>From the traditional “clunky AI” to today’s “AI assistant,” the evolution of voice dialogue systems has been nothing short of astonishing. <strong>But just how far are we from achieving truly “natural human-machine dialogue”?</strong> While current voice models excel in technical metrics, they still lack a certain “human touch.” They may recognize single emotions like “happiness” or “sadness,” but struggle to truly understand the complexity of our emotional changes or empathize with our situations. They may engage in fluent one-on-one exchanges, yet become flustered in real-world interaction scenarios such as interruptions, overlapping speech, or group chats. This is the “uncanny valley” that current voice dialogue systems struggle to cross.</p>
<p>To break through this bottleneck and advance technology toward truly “human-like” interaction, a coalition of institutions—including <strong>Northwestern Polytechnical University, Nanjing University, The Chinese University of Hong Kong, Huawei Technologies Co., Ltd., and AISHELL</strong>—has jointly launched the HumDial (Human-like Spoken Dialogue Systems) Challenge! We believe a truly intelligent dialogue system must not only “understand clearly, reason logically, and express coherently” but also possess the ability to interact seamlessly with humans in real, emotionally complex environments.</p>
<p>The inaugural HumDial2026 Challenge will be held at ICASSP 2026, a premier conference for speech research, and will focus on two core challenges:</p>
<ul>
<li><strong>Emotional Intelligence:</strong> Moving beyond simplistic emotion labeling, this track will test a model&rsquo;s ability to accurately understand context-dependent emotions, provide empathetic responses, conduct in-depth reasoning, and dynamically track emotional shifts—empowering AI to truly understand and connect with users.</li>
<li><strong>Full-Duplex Interaction:</strong> Breaking free from rigid turn-based exchanges, this track will evaluate a system&rsquo;s ability to handle interruptions, overlapping speech, real-time feedback, and natural conversational rhythms, helping AI learn to communicate more naturally.</li>
</ul>
<p>We will not only introduce brand-new evaluation dimensions but also release exclusive, finely annotated datasets of real-world scenarios for each track. If you’re passionate about “human-like” dialogue systems and eager to shape the future of next-generation voice interaction, we welcome you to follow and register for the challenge! Let’s work together to turn AI into a warm, emotionally aware communication partner.</p>


                                </article>
                            </div>
                        </div>

                    
                </div>
            </section><div id="footer">
    <div class="container">
        <div class="row">
        </div>
    </div>

    <ul class="icons">
    </ul>

    <div class="copyright">
        <ul class="menu">
            
                <li>Design: <a href="https://html5up.net">HTML5 UP</a>
                <li><a href="https://github.com/half-duplex/hugo-arcana">Theme</a>
        </ul>
    </div>
</div>
</div><script src="/HumDial-Challenge/js/jquery.min.js"></script>
<script src="/HumDial-Challenge/js/jquery.dropotron.min.js"></script>
<script src="/HumDial-Challenge/js/browser.min.js"></script>
<script src="/HumDial-Challenge/js/breakpoints.min.js"></script>
<script src="/HumDial-Challenge/js/util.js"></script>
<script src="/HumDial-Challenge/js/main.js"></script>
</body>
</html>
