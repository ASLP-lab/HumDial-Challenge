<!DOCTYPE HTML>
<html lang="en">
    <head>
	<meta name="generator" content="Hugo 0.103.1" />

<title>ICASSP 2026: Human-like Spoken Dialogue Systems Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="/HumDial-Challenge/style.css" />
<meta property="og:title" content="Home" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://aslp-lab.github.io/HumDial-Challenge/" />

</head>
    <body class="is-preload">
        <div id="page-wrapper"><div id="header">
    <h1><a href="https://aslp-lab.github.io/HumDial-Challenge/" id="logo">
        Human-like-Spoken-Dialogue-Systems-Challenge - ICASSP 2026
    </a></h1>

    <nav id="nav">
        <ul>
                <li class="current">
                    <a href="/HumDial-Challenge/">Home</a>
                <li class="">
                    <a href="/HumDial-Challenge/dataset/">Dataset</a>
                <li class="">
                    <a href="#">Track 1: Emotion Intelligence</a><ul>
                        <li class="">
                            <a href="/HumDial-Challenge/task1/description">description</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task1/test_set">test_set</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task1/leaderboard">leaderboard</a>
                    </ul>
                <li class="">
                    <a href="#">Track 2: Full-Duplex Interaction</a><ul>
                        <li class="">
                            <a href="/HumDial-Challenge/task2/description">description</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task2/test_set">test_set</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task2/leaderboard">leaderboard</a>
                    </ul>
                <li class="">
                    <a href="/HumDial-Challenge/faq/">FAQ</a>
        </ul>
    </nav>
</div><section class="wrapper style2">
    <div class="container">
        <header class="major">
            <h2>Human-like-Spoken-Dialogue-Systems-Challenge</h2><p>The challenge aims to promote systematic, real-world evaluation of next-generation dialogue systems and advance the field toward truly human-like interaction.</p>
        </header>
    </div>
</section>
<section class="wrapper style1">
    <div class="container">
        
            <div class=""><!-- raw HTML omitted -->
<h2 id="challenge-call">Challenge Call</h2>
<p>Have you been following the recent buzz around the impressive performance of next-generation voice dialogue models like GPT-4o, Doubao, and the newly released GPT-Realtime? They are not only lightning-fast and expressive but also enable seamless multimodal interactions, making conversations feel remarkably human.</p>
<p>From the traditional “clunky AI” to today’s “AI assistant,” the evolution of voice dialogue systems has been nothing short of astonishing. <strong>But just how far are we from achieving truly “natural human-machine dialogue”?</strong> While current voice models excel in technical metrics, they still lack a certain “human touch.” They may recognize single emotions like “happiness” or “sadness,” but struggle to truly understand the complexity of our emotional changes or empathize with our situations. They may engage in fluent one-on-one exchanges, yet become flustered in real-world interaction scenarios such as interruptions, overlapping speech, or group chats. This is the “uncanny valley” that current voice dialogue systems struggle to cross.</p>
<p>To break through this bottleneck and advance technology toward truly “human-like” interaction, a coalition of institutions—including <strong>Northwestern Polytechnical University, Nanjing University, The Chinese University of Hong Kong, Huawei Technologies Co., Ltd., and AISHELL</strong>—has jointly launched the HumDial (Human-like Spoken Dialogue Systems) Challenge! We believe a truly intelligent dialogue system must not only “understand clearly, reason logically, and express coherently” but also possess the ability to interact seamlessly with humans in real, emotionally complex environments.</p>
<p>The inaugural HumDial2026 Challenge will be held at ICASSP 2026, a premier conference for speech research, and will focus on two core challenges:</p>
<ul>
<li><strong>Emotional Intelligence:</strong> Moving beyond simplistic emotion labeling, this track will test a model&rsquo;s ability to accurately understand context-dependent emotions, provide empathetic responses, conduct in-depth reasoning, and dynamically track emotional shifts—empowering AI to truly understand and connect with users.</li>
<li><strong>Full-Duplex Interaction:</strong> Breaking free from rigid turn-based exchanges, this track will evaluate a system&rsquo;s ability to handle interruptions, overlapping speech, real-time feedback, and natural conversational rhythms, helping AI learn to communicate more naturally.</li>
</ul>
<p>We will not only introduce brand-new evaluation dimensions but also release exclusive, finely annotated datasets of real-world scenarios for each track. If you’re passionate about “human-like” dialogue systems and eager to shape the future of next-generation voice interaction, we welcome you to follow and register for the challenge! Let’s work together to turn AI into a warm, emotionally aware communication partner.</p>
</div>
            <div class=""><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="registration">Registration</h2>
<p>Teams can register by the google form: <a href="https://docs.google.com/forms/d/e/1FAIpQLSdRrlfqrhh8QhOxtKMr03AxnnX14md_EwFuIuMt-Hf4fhhARA/viewform?usp=header">https://docs.google.com/forms/d/e/1FAIpQLSdRrlfqrhh8QhOxtKMr03AxnnX14md_EwFuIuMt-Hf4fhhARA/viewform?usp=header</a></p>
<p><strong>Reminder!</strong> Please use your institutional or corporate email address to register, and avoid using personal email accounts.</p>
</div>
            <div class=""><h2 id="timeline">Timeline</h2>
<ul>
<li><strong>August 20, 2025</strong>: Registration opens</li>
<li><strong>September 29, 2025</strong>: Release of training set, validation set, and baseline system(delay for a few days)</li>
<li><strong>November 10, 2025</strong>: Release of test set</li>
<li><strong>November 25, 2025</strong>: Submission deadline</li>
<li><strong>December 7, 2025</strong>: Deadline for submitting 2-page papers to ICASSP 2026 (invited teams only)</li>
<li><strong>January 11, 2026</strong>: Notification of acceptance for 2-page ICASSP 2026 papers</li>
<li><strong>January 18, 2026</strong>: Submission of final version of papers</li>
<li><strong>May 4–8, 2026</strong>: ICASSP 2026 Conference, Barcelona, Spain</li>
</ul>
<h2 id="baseline">Baseline</h2>
<ol>
<li><strong>Emotional Intelligence:</strong>: Comming soon</li>
</ol>
<!-- raw HTML omitted -->
<ol start="2">
<li><strong>Full-Duplex Interaction</strong>: The competition provides a baseline system built upon <a href="https://github.com/ASLP-lab/Easy-Turn">Easy Turn</a> and <a href="https://github.com/ASLP-lab/OSUM/tree/main/OSUM-EChat">OSUM-EChat</a>.This baseline serves as a reproducible and extensible starting point, helping participants better benchmark their systems and ensuring fair comparison across different approaches.</li>
</ol>
<h2 id="guidelines-for-participants">Guidelines for participants</h2>
<h3 id="1-general-principles-for-resource-usage">1. General Principles for Resource Usage</h3>
<p>To ensure the fairness, impartiality and transparency of the competition, all participating teams must strictly abide by the following resource usage regulations.</p>
<h4 id="11-resource-definition">1.1 Resource Definition</h4>
<ul>
<li><strong>Internal Resources</strong>: Official datasets, baseline models and related documentation directly provided by the organizers.</li>
<li><strong>External Resources</strong>: Any resources not provided by the organizers, including but not limited to external data, pre-trained models, open-source tool libraries and third-party API services.</li>
</ul>
<h4 id="12-external-resource-usage-policy">1.2 External Resource Usage Policy</h4>
<p><strong>External Data</strong></p>
<ul>
<li><strong>Core Requirements</strong>: Must be publicly available datasets. Any researcher or group can directly download or obtain the data through standard application processes via public, free channels (such as official websites, academic data platforms, open-source communities).</li>
<li><strong>Strictly Prohibited</strong>: The use of any private, non-public or access-restricted proprietary datasets is strictly prohibited.</li>
</ul>
<p><strong>External Pre-trained Models</strong></p>
<ul>
<li><strong>Core Requirements</strong>: Must be publicly available open-source models.</li>
<li><strong>Permitted Scope</strong>: The use of any open-source pre-trained models available through public channels (such as Hugging Face, GitHub, model official websites, etc.) is allowed, along with clear version information.</li>
</ul>
<h4 id="13-declaration-obligation">1.3 Declaration Obligation</h4>
<p>Participating teams must clearly and completely declare all used resources and usage methods (including internal and external) in the form of a list in the final technical report.</p>
<h3 id="2-competition-dataset-usage-policy">2. Competition Dataset Usage Policy</h3>
<p><strong>Train Set</strong>: Participants may use the official train subset.</p>
<ul>
<li>Any form of regular data augmentation based on the official train set is allowed (such as adding noise, pitch shifting, speed variation, etc.).</li>
<li>The use of external public datasets that fully comply with the regulations in Section 1.2 is permitted for supplementary training.</li>
<li>If new data is synthesized through models (for example, using TTS technology to generate speech), the synthesis model (such as TTS model) itself must be a publicly available open-source model that complies with Section 1.2.</li>
<li>All data augmentation and synthesis methods, processes and external data sources used must be fully explained in the final technical report.</li>
</ul>
<p><strong>Dev Set</strong>: Can be used for model performance evaluation and debugging.</p>
<p><strong>Test Set</strong>: Competition rankings will be based on model performance on the test set. The organizers will provide a public test set for participants to verify, but the final ranking will be based on the organizers&rsquo; combined results of the public test set and successfully reproduced results on the hidden test set.</p>
<h3 id="3-submission-requirements">3. Submission Requirements</h3>
<p>To ensure the fairness and reproducibility of the competition, all participating teams must submit a complete, independently runnable final deliverable package within the specified time.</p>
<h4 id="31-submission-content-list">3.1 Submission Content List</h4>
<p>The submission package of participating teams must include all of the following contents:</p>
<ul>
<li><strong>Final Results File</strong>: Model inference result file generated in the specified format (for example, submission.json).</li>
<li><strong>Model Files</strong>: Complete model weights, configuration files and related dependencies.</li>
<li><strong>Docker Image</strong>: A Docker image with configured environment that supports one-click inference execution.</li>
<li><strong>Technical Documentation</strong>: A detailed README.md document clearly explaining how to use the provided code and model files to reproduce the competition results.</li>
</ul>
<h4 id="32-source-code-requirements">3.2 Source Code Requirements</h4>
<ul>
<li><strong>Reproduction Script</strong>: A one-click startup script (such as run.sh) must be provided to execute the complete inference process and generate the final result file.</li>
</ul>
<h4 id="33-docker-image-specifications">3.3 Docker Image Specifications</h4>
<ul>
<li><strong>Environment Consistency</strong>: The Docker image must contain all necessary environments, software libraries and dependencies to ensure smooth operation in the evaluation environment.</li>
<li><strong>One-click Inference</strong>: The image must contain an executable script that can automatically complete all inference steps and output the result file as required when called.</li>
<li><strong>Detailed Guide</strong>: Specific specifications for building, using and submitting Docker images will be provided uniformly after the baseline implementation is released.</li>
</ul>
<h3 id="4-competition-supervision-and-right-of-interpretation">4. Competition Supervision and Right of Interpretation</h3>
<h4 id="41-right-of-interpretation">4.1 Right of Interpretation</h4>
<p>The final right of interpretation of all rules of this competition belongs to the organizers. During the competition, the organizers have the right to adjust or supplement the rules according to actual circumstances, and will notify all participating teams in a timely manner.</p>
<h4 id="42-fairness-and-authenticity-verification">4.2 Fairness and Authenticity Verification</h4>
<p>To ensure the fairness and impartiality of the competition, the organizers reserve the right to review participating works. If necessary, the organizers have the right to request participating teams to provide more detailed technical details, and participating teams must cooperate.</p>
<h4 id="43-violation-handling">4.3 Violation Handling</h4>
<p>If participating teams exhibit any of the following behaviors, the organizers have the right to unilaterally cancel their participation qualification, winning qualification and recover bonuses and prizes:</p>
<ul>
<li>Submitted works contain false or forged information.</li>
<li>Serious violations of competition rules or submission requirements.</li>
<li>Use of any form of cheating in the competition.</li>
</ul>
</div>
            <div class=""><h2 id="organizers">Organizers</h2>
<p>The challenge is organized by a distinguished team of researchers:</p>
<ul>
<li><strong>Lei Xie</strong>, Professor, Northwestern Polytechnical University</li>
<li><strong>Shuai Wang</strong>, Associate Professor, Nanjing University</li>
<li><strong>Haizhou Li</strong>, Professor, Chinese University of Hong Kong</li>
<li><strong>Eng Siong Chng</strong>, Professor, Nanyang Technological University</li>
<li><strong>Hung-yi Lee</strong>, Professor, Natioanl Taiwan University</li>
<li><strong>Chao Zhang</strong>, Assistant Professor, Tsinghua University</li>
<li><strong>Guangzhi Sun</strong>, Junior Research Fellow, University of Cambridge</li>
<li><strong>Xixin Wu</strong>, Assistant Professor, Chinese University of Hong Kong</li>
<li><strong>Longshuai Xiao</strong>, Huawei Technologies</li>
<li><strong>Zihan Zhang</strong>, Huawei Technologies</li>
<li><strong>Xinsheng Wang</strong>, Soul AI Lab</li>
<li><strong>Hui Bu</strong>, AISHELL</li>
<li><strong>Xin Xu</strong>， AISHELL</li>
<li><strong>Zhixian Zhao</strong>, Northwestern Polytechnical University</li>
<li><strong>Hongfei Xue</strong>, Northwestern Polytechnical University</li>
<li><strong>Xuelong Geng</strong>, Northwestern Polytechnical University</li>
<li><strong>GuoJian Li</strong>, Northwestern Polytechnical University</li>
<li><strong>Shuiyuan Wang</strong>, Northwestern Polytechnical University</li>
</ul>
<h2 id="contact">Contact</h2>
<p>For any inquiries, please contact:</p>
<ul>
<li><strong>Track 1: Emotion Intelligence</strong><br>
Email: <a href="mailto:zxzhao@mail.nwpu.edu.cn">zxzhao@mail.nwpu.edu.cn</a>, <a href="mailto:wangshuiyuan@mail.nwpu.edu.cn">wangshuiyuan@mail.nwpu.edu.cn</a></li>
<li><strong>Track 2: Full-Duplex Interaction</strong><br>
Email: <a href="mailto:aslp_lgj@mail.nwpu.edu.cn">aslp_lgj@mail.nwpu.edu.cn</a>, <a href="mailto:hfxue@mail.nwpu.edu.cn">hfxue@mail.nwpu.edu.cn</a></li>
</ul>
<p>Welcome to join our <strong>WeChat group</strong></p>
<figure><img src="./images/wechat.jpeg" width="300"/>
</figure>

</div>
    </div>
</section>
<section id="cta" class="wrapper style3">
    <div class="container">
        <header>
            <h2>Are you ready?</h2>
                <a href="task1" class="button">Get started with track 1</a>
        </header>
    </div>
</section>
<div id="footer">
    <div class="container">
        <div class="row">
        </div>
    </div>

    <ul class="icons">
    </ul>

    <div class="copyright">
        <ul class="menu">
            
                <li>Design: <a href="https://html5up.net">HTML5 UP</a>
                <li><a href="https://github.com/half-duplex/hugo-arcana">Theme</a>
        </ul>
    </div>
</div>
</div><script src="/HumDial-Challenge/js/jquery.min.js"></script>
<script src="/HumDial-Challenge/js/jquery.dropotron.min.js"></script>
<script src="/HumDial-Challenge/js/browser.min.js"></script>
<script src="/HumDial-Challenge/js/breakpoints.min.js"></script>
<script src="/HumDial-Challenge/js/util.js"></script>
<script src="/HumDial-Challenge/js/main.js"></script>
</body>
</html>
