<!DOCTYPE HTML>
<html lang="en">
    <head>
	<meta name="generator" content="Hugo 0.103.1" />

<title>ICASSP 2026: Human-like Spoken Dialogue Systems Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="/HumDial-Challenge/style.css" />
<meta property="og:title" content="Home" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://aslp-lab.github.io/HumDial-Challenge/" />

</head>
    <body class="is-preload">
        <div id="page-wrapper"><div id="header">
    <h1><a href="https://aslp-lab.github.io/HumDial-Challenge/" id="logo">
        Human-like-Spoken-Dialogue-Systems-Challenge - ICASSP 2026
    </a></h1>

    <nav id="nav">
        <ul>
                <li class="current">
                    <a href="/HumDial-Challenge/">Home</a>
                <li class="">
                    <a href="/HumDial-Challenge/dataset/">Dataset</a>
                <li class="">
                    <a href="#">Task 1: Emotion Intelligence</a><ul>
                        <li class="">
                            <a href="/HumDial-Challenge/task1/description">description</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task1/test_set">test_set</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task1/leaderboard">leaderboard</a>
                    </ul>
                <li class="">
                    <a href="#">Task 2: Full-Duplex Interaction</a><ul>
                        <li class="">
                            <a href="/HumDial-Challenge/task2/description">description</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task2/test_set">test_set</a>
                        <li class="">
                            <a href="/HumDial-Challenge/task2/leaderboard">leaderboard</a>
                    </ul>
        </ul>
    </nav>
</div><section class="wrapper style2">
    <div class="container">
        <header class="major">
            <h2>Human-like-Spoken-Dialogue-Systems-Challenge</h2><p>The challenge aims to promote systematic, real-world evaluation of next-generation dialogue systems and advance the field toward truly human-like interaction.</p>
        </header>
    </div>
</section>
<section class="wrapper style1">
    <div class="container">
        
            <div class=""><figure><img src="../../images/demo.png"/><figcaption>
            <h4>HumDial Challenge Banner</h4>
        </figcaption>
</figure>

<h2 id="challenge-call">Challenge Call</h2>
<p>Have you also been seeing endless buzz lately about the amazing performance of next-generation voice dialogue models like GPT-4o, Doubao, and the newly released GPT-Realtime? Not only are they lightning-fast in response and expressive in voice, but they also enable smooth multimodal interactions—making it feel as if we’re truly conversing with a &ldquo;real person.&rdquo; From the traditional &ldquo;clunky AI&rdquo; to today’s &ldquo;AI assistant,&rdquo; the evolution of voice dialogue systems has been nothing short of astonishing. But just how far are we from achieving truly &ldquo;natural human-machine dialogue&rdquo;?</p>
<p>While current voice models excel in technical metrics, they still lack a certain &ldquo;human touch.&rdquo; They may recognize single emotions like &ldquo;happiness&rdquo; or &ldquo;sadness,&rdquo; but struggle to truly understand the complexity of our emotional changes or empathize with our situations. They may engage in fluent one-on-one exchanges, yet become flustered in real-world interaction scenarios such as interruptions, overlapping speech, or group chats. This is precisely the &ldquo;uncanny valley&rdquo; that current voice dialogue systems find hard to cross.</p>
<p>To break through this bottleneck and advance technology toward truly &ldquo;human-like&rdquo; interaction, a coalition of institutions—including Northwestern Polytechnical University, Nanjing University, The Chinese University of Hong Kong, Huawei Technologies Co., Ltd., and ShellTech—has jointly launched the HumDial (Human-like Spoken Dialogue Systems) Challenge!</p>
<p>We believe a truly intelligent dialogue system must not only &ldquo;understand clearly, reason logically, and express coherently&rdquo; but also possess the ability to interact seamlessly with humans in real, emotionally complex environments.</p>
<p>The inaugural 2026 HumDial Challenge will be held at ICASSP 2026, the top-tier conference for speech research, focusing on two core challenges:</p>
<p>Emotional Intelligence: No more simplistic &ldquo;emotion labeling&rdquo;! This track will test models’ capabilities in accurately understanding context-dependent emotions, providing empathetic responses, conducting in-depth reasoning, and dynamically tracking emotional shifts—empowering AI to truly &ldquo;know your heart.&rdquo;
Full-Duplex Interaction: Say goodbye to rigid &ldquo;you speak, then I speak&rdquo; exchanges! This track aims to evaluate systems’ ability to handle interruptions, overlapping speech, real-time feedback, and natural conversation rhythms—helping AI learn to &ldquo;communicate naturally.&rdquo;</p>
<p>We will not only introduce brand-new evaluation dimensions but also release exclusive, finely annotated datasets of real-world scenarios for each track.</p>
<p>If you’re passionate about &ldquo;human-like&rdquo; dialogue systems and eager to shape the future of next-generation voice interaction, we welcome you to follow and register for the challenge! Let’s work together to turn AI into a warm, emotionally aware communication partner.</p>
<!-- raw HTML omitted -->
</div>
            <div class=""><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="registration">Registration</h2>
<p>Teams can register by the google form: <a href="https://docs.google.com/forms/d/e/1FAIpQLSdRrlfqrhh8QhOxtKMr03AxnnX14md_EwFuIuMt-Hf4fhhARA/viewform?usp=header">https://docs.google.com/forms/d/e/1FAIpQLSdRrlfqrhh8QhOxtKMr03AxnnX14md_EwFuIuMt-Hf4fhhARA/viewform?usp=header</a></p>
</div>
            <div class=""><h2 id="timeline">Timeline</h2>
<ul>
<li><strong>Aug 20, 2025</strong>:Registration opens;</li>
<li><strong>Sep 10, 2025</strong>: The training set,validation set,data synthesis pipeline, and baseline systems are released.</li>
<li><strong>Nov 01, 2025</strong>: Test set released</li>
<li><strong>Nov 15, 2025</strong>: Submission deadline for both tracks.</li>
<li><strong>Dec 7, 2026</strong>: Grand challenge 2-page paper deadline (invited teams only).</li>
<li><strong>Jan 11, 2026</strong>: Grand challenge 2-page paper acceptance notification.</li>
<li><strong>Jan 18, 2026</strong>: Camera-ready Grand Challenge 2-page Papers Due</li>
</ul>
<h2 id="guidelines-for-participants">Guidelines for participants</h2>
<ol>
<li>Model Requirements: Participants may submit systems based on either end-to-end architectures or cascaded pipelines . There is no restriction on the model structure, but all models must be trained using publicly available resources.</li>
<li>Data Usage: Use of the official test set or any of its labels for model training or tuning is strictly prohibited. Participants are not allowed to use any private or unauthorized datasets.</li>
<li>Submission Format: Participants must submit a docker container with the complete system and source code along with detailed instructions for reproduction. All submissions must be executable and allow for transparent verification by the organizers.</li>
<li>Prizes and Awards: The top 3 teams in each track will receive prizes based on final rankings: 5,000 USD for 1st place, 3,000 USD for 2nd place, and 2,000 USD for 3rd place. Winning teams will be invited to present their work at the ICASSP 2026 special session</li>
<li>Final Interpretation: The organizing committee reserves the right of final interpretation of the rules and all matters related to the challenge.</li>
</ol>
</div>
            <div class=""><h2 id="organizers">Organizers</h2>
<p>The challenge is organized by a distinguished team of researchers:</p>
<ul>
<li><strong>Lei Xie</strong>, Professor, Northwestern Polytechnical University</li>
<li><strong>Shuai Wang</strong>, Associate Professor, Nanjing University</li>
<li><strong>Haizhou Li</strong>, Professor, Chinese University of Hong Kong</li>
<li><strong>Eng Siong Chng</strong>, Professor, Nanyang Technological University</li>
<li><strong>Hung-yi Lee</strong>, Professor, Natioanl Taiwan University</li>
<li><strong>Chao Zhang</strong>, Assistant Professor, Tsinghua University</li>
<li><strong>Guangzhi Sun</strong>, Junior Research Fellow, University of Cambridge</li>
<li><strong>Xixin Wu</strong>, Assistant Professor, Chinese University of Hong Kong</li>
<li><strong>Longshuai Xiao</strong>, Huawei Technologies</li>
<li><strong>Zihan Zhang</strong>, Huawei Technologies</li>
<li><strong>Xinsheng Wang</strong>, Soul AI Lab</li>
<li><strong>Hui Bu</strong>, AISHELL</li>
<li><strong>Xin Xu</strong>， AISHELL</li>
<li><strong>Zhixian Zhao</strong>, Northwestern Polytechnical University</li>
<li><strong>Hongfei Xue</strong>, Northwestern Polytechnical University</li>
<li><strong>Xuelong Geng</strong>, Northwestern Polytechnical University</li>
<li><strong>GuoJian Li</strong>, Northwestern Polytechnical University</li>
<li><strong>Shuiyuan Wang</strong>, Northwestern Polytechnical University</li>
</ul>
<h2 id="contact">Contact</h2>
<p>For any inquiries, please contact:
Email: <a href="mailto:hfxue@mail.nwpu.edu.cn">hfxue@mail.nwpu.edu.cn</a>, <a href="mailto:zxzhao@mail.nwpu.edu.cn">zxzhao@mail.nwpu.edu.cn</a>, <a href="mailto:aslp_lgj@mail.nwpu.edu.cn">aslp_lgj@mail.nwpu.edu.cn</a>, <a href="mailto:wangshuiyuan@mail.nwpu.edu.cn">wangshuiyuan@mail.nwpu.edu.cn</a></p>
</div>
    </div>
</section>
<section id="cta" class="wrapper style3">
    <div class="container">
        <header>
            <h2>Are you ready?</h2>
                <a href="task1" class="button">Get started with task 1</a>
        </header>
    </div>
</section>
<div id="footer">
    <div class="container">
        <div class="row">
        </div>
    </div>

    <ul class="icons">
    </ul>

    <div class="copyright">
        <ul class="menu">
            
                <li>Design: <a href="https://html5up.net">HTML5 UP</a>
                <li><a href="https://github.com/half-duplex/hugo-arcana">Theme</a>
        </ul>
    </div>
</div>
</div><script src="/HumDial-Challenge/js/jquery.min.js"></script>
<script src="/HumDial-Challenge/js/jquery.dropotron.min.js"></script>
<script src="/HumDial-Challenge/js/browser.min.js"></script>
<script src="/HumDial-Challenge/js/breakpoints.min.js"></script>
<script src="/HumDial-Challenge/js/util.js"></script>
<script src="/HumDial-Challenge/js/main.js"></script>
</body>
</html>
