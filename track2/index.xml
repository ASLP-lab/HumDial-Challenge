<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Track2s on ICASSP 2026: Human-like Spoken Dialogue Systems Challenge</title>
    <link>https://aslp-lab.github.io/HumDial-Challenge/track2/</link>
    <description>Recent content in Track2s on ICASSP 2026: Human-like Spoken Dialogue Systems Challenge</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://aslp-lab.github.io/HumDial-Challenge/track2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/track2/results/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/track2/results/</guid>
      <description>Full-Duplex Interaction Track Results 1. Task Dimensions and Evaluation Methodology The final score for this challenge track is derived using an automated evaluation script, details of which can be found in Full-Duplex_Interaction_Evaluation. The Total Score comprises three components: the Interruption Total Score, the Rejection Total Score, and the Total Delay Score.
Interruption Total Score: Calculated as the average score of the Chinese (CN) interruption test set and the English (EN) interruption test set.</description>
    </item>
    
    <item>
      <title>Test Set Track 2</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/track2/test_set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/track2/test_set/</guid>
      <description>Test Set Description The test set (HD-Track2-Test) has been officially released and contains two subsets: one in Chinese and one in English. Each language subset covers two major scenarios—Interruption and Rejection. The dataset consists of 9,100 samples in total, including:
5,000 official test samples: test-{dialogue_id}.wav (Chinese/English ratio = 1:1) 4,100 clean audio files required for scoring interruption and rejection: clean-{dialogue_id}.wav Note: The data has been randomly shuffled. The test.wav and clean.</description>
    </item>
    
    <item>
      <title>Track 2: Full-Duplex Interaction</title>
      <link>https://aslp-lab.github.io/HumDial-Challenge/track2/description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aslp-lab.github.io/HumDial-Challenge/track2/description/</guid>
      <description>Challenge Tasks The full-duplex benchmark primarily encompasses two major Scenarios: interruption and rejection.
1. Interruption Scenarios: Follow-up Questions: The user poses a follow-up question based on the model’s previous response, interrupting the ongoing output. The model should promptly address the user’s follow-up inquiry. Negation / Dissatisfaction: The user expresses dissatisfaction or disagreement with the model’s response using negative statements, interrupting the model mid-sentence. The model should appropriately react to the user’s negation or dissatisfaction in a timely manner.</description>
    </item>
    
  </channel>
</rss>
