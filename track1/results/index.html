<!DOCTYPE HTML>
<html lang="en">
    <head>

<title>Results | ICASSP 2026: Human-like Spoken Dialogue Systems Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="/HumDial-Challenge/style.css" />
<meta property="og:title" content="Results" />
<meta property="og:description" content="Emotional Intelligence Track Results 1. Task Dimensions &amp; Evaluation Methodology The final score for this challenge track combines both automated metrics and human evaluation to ensure the results are professional and objective.
Evaluation Environment: The automated evaluation utilizes the Qwen/Qwen3-Omni-30B-A3B-Instruct model, which is deployed entirely in a local environment for scoring. For the detailed evaluation prompts, please refer to the competition guidelines.
Task 1: Emotional Trajectory Detection
Dimension 1: Accuracy_Completeness Dimension 2: Depth_Granularity Dimension 3: Added_Value Task 2: Emotional Reasoning" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aslp-lab.github.io/HumDial-Challenge/track1/results/" /><meta property="article:section" content="track1" />



</head>
    <body class="is-preload">
        <div id="page-wrapper"><div id="header">
    <h1><a href="https://aslp-lab.github.io/HumDial-Challenge/" id="logo">
        ICASSP 2026 Human-like Spoken Dialogue Systems Challenge
    </a></h1>

    <nav id="nav">
        <ul>
                <li class="">
                    <a href="/HumDial-Challenge/">Home</a>
                <li class="">
                    <a href="#">Track 1: Emotional Intelligence</a><ul>
                        <li class="">
                            <a href="/HumDial-Challenge/track1/description">Description</a>
                        <li class="">
                            <a href="/HumDial-Challenge/track1/test_set">Test_set</a>
                        <li class="">
                            <a href="/HumDial-Challenge/track1/results">Results</a>
                    </ul>
                <li class="">
                    <a href="#">Track 2: Full-Duplex Interaction</a><ul>
                        <li class="">
                            <a href="/HumDial-Challenge/track2/description">Description</a>
                        <li class="">
                            <a href="/HumDial-Challenge/track2/test_set">Test_set</a>
                        <li class="">
                            <a href="/HumDial-Challenge/track2/results">Results</a>
                    </ul>
                <li class="">
                    <a href="/HumDial-Challenge/faq/">FAQ</a>
        </ul>
    </nav>
</div>

            <section class="wrapper style1">
                <div class="container">
                    

                        <div class="">
                            <div id="content">
                                <article>
    <header>
        <h2>Results</h2>
        <p></p>
        
        
        <ul class="tags">
</ul>

    </header><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="emotional-intelligence-track-results">Emotional Intelligence Track Results</h3>
<h4 id="1-task-dimensions--evaluation-methodology">1. Task Dimensions &amp; Evaluation Methodology</h4>
<p>The final score for this challenge track combines both automated metrics and human evaluation to ensure the results are professional and objective.</p>
<p><strong>Evaluation Environment</strong>: The automated evaluation utilizes the Qwen/Qwen3-Omni-30B-A3B-Instruct model, which is deployed entirely in a local environment for scoring. For the detailed evaluation prompts, please refer to the <a href="https://github.com/ASLP-lab/Hum-Dial">competition guidelines</a>.</p>
<p><strong>Task 1: Emotional Trajectory Detection</strong></p>
<ul>
<li>Dimension 1: Accuracy_Completeness</li>
<li>Dimension 2: Depth_Granularity</li>
<li>Dimension 3: Added_Value</li>
</ul>
<p><strong>Task 2: Emotional Reasoning</strong></p>
<ul>
<li>Dimension 1: Information_Integration</li>
<li>Dimension 2: Insight_RootCause</li>
<li>Dimension 3: Clarity_Logic</li>
</ul>
<p><strong>Task 3: Empathy Assessment</strong></p>
<ul>
<li>Dimension 1: Textual_empathy_insight</li>
<li>Dimension 2: Vocal_empathy_congruence</li>
<li>Dimension 3: Audio_quality_naturalness</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">**Task Dimension**</th>
<th style="text-align:center">**Evaluation Method**</th>
<th style="text-align:center">**Evaluation Tool &amp; Team**</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">**Task 1: Emotional Trajectory Detection**</td>
<td style="text-align:center">Automated Evaluation</td>
<td style="text-align:center">Qwen/Qwen3-Omni-30B-A3B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">**Task 2: Emotional Reasoning**</td>
<td style="text-align:center">Automated Evaluation</td>
<td style="text-align:center">Qwen/Qwen3-Omni-30B-A3B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">**Task 3: Empathy Assessment - Dimension 1**</td>
<td style="text-align:center">Automated Evaluation</td>
<td style="text-align:center">Qwen/Qwen3-Omni-30B-A3B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">**Task 3: Empathy Assessment - Dimensions 2 &amp; 3**</td>
<td style="text-align:center">Human Evaluation</td>
<td style="text-align:center">20 Human Evaluators</td>
</tr>
</tbody>
</table>
<p>⚠️ <strong>Rules on Violations &amp; Anomalies</strong>
To maintain fairness and validity, the following rules strictly apply:</p>
<ol>
<li><strong>Language Mismatch</strong>: For both the Chinese and English test sets, if the language of a submitted response does not match the input audio, that sample will be automatically assigned the minimum score (1 point).</li>
<li><strong>Human Evaluation Team</strong>
<strong>The evaluation for Dimensions 2 and 3 of Task 3</strong> is conducted by human evaluators organized by Beijing AIShell Co., Ltd. The composition and qualifications of the evaluators are detailed below:</li>
</ol>
<ul>
<li><strong>Number of Evaluators</strong>: 20 in total.</li>
<li><strong>Language Groups</strong>: 10 for the Chinese evaluation group, 10 for the English evaluation group.</li>
<li><strong>Experience &amp; Education</strong>: All evaluators possess a university Bachelor&rsquo;s degree or higher and have over six months of relevant data-annotation/subjective-evaluation experience.</li>
<li><strong>Language Proficiency</strong>: the Chinese evaluation group is composed of native Mandarin speakers. Evaluators assigned to the English test set possess fluent English proficiency.</li>
<li><strong>Demographics</strong>:
<ul>
<li><strong>Gender Distribution</strong>: 13 female, 7 male.</li>
<li><strong>Age Distribution</strong>: Average age 24.2 years (Range: 22–27 years).</li>
</ul>
</li>
</ul>
<h4 id="2-final-scores-and-ranking">2. Final Scores and Ranking</h4>
<p>The <strong>Final Score</strong> for each team is calculated as the weighted sum of scores from the respective task dimensions:
$$\text{Final Score(zh/en)} = (\text{Task-1-Avg} \times 0.2) + (\text{Task-2-Avg} \times 0.2) + (\text{Task-3-D1} \times 0.1) + (\text{Task-3-D2} \times 0.25) + (\text{Task-3-D3} \times 0.25)$$
$$\text{Final Score} = (\text{Final Score(zh)} \times 0.5) + (\text{Final Score(en)} \times 0.5)$$</p>
<figure><img src="./images/HD-Track1-Test.png" width="900"/>
</figure>

<figure><img src="./images/HD-Track1-Test_Final_score.png" width="300"/>
</figure>

<h5 id="final-score">Final Score</h5>
<table>
<thead>
<tr>
<th>Team</th>
<th>Final Score</th>
<th>Ranking</th>
</tr>
</thead>
<tbody>
<tr>
<td>TeleAI*</td>
<td>4.27</td>
<td>1</td>
</tr>
<tr>
<td>Tencent Ai Lab-NJU*</td>
<td>4.24</td>
<td>2</td>
</tr>
<tr>
<td>BJTU_Unisound_team*</td>
<td>4.21</td>
<td>3</td>
</tr>
<tr>
<td>SenseDialog</td>
<td>4.06</td>
<td>4</td>
</tr>
<tr>
<td>HDTLAB</td>
<td>3.86</td>
<td>5</td>
</tr>
<tr>
<td>IUSpeech</td>
<td>3.14</td>
<td>6</td>
</tr>
<tr>
<td>Baseline</td>
<td>3.06</td>
<td>7</td>
</tr>
<tr>
<td>Lingcon insight</td>
<td>2.96</td>
<td>8</td>
</tr>
<tr>
<td>*: invited to submit ICASSP 2-page papers.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="3-icassp-2-page-papers">3. ICASSP 2-Page Papers</h3>
<p>According to the grand challenge official rules, the <strong>top 3 teams in this track (TeleAI, Tencent Ai Lab-NJU and BJTU_Unisound_team)</strong> are invited to submit papers (2 pages main content + extra page with refs) to the ICASSP 2026 Grand Challenge Track.</p>
<ul>
<li>Specific submission instructions will be emailed separately to the qualifying teams.</li>
</ul>


                                </article>
                            </div>
                        </div>

                    
                </div>
            </section><div id="footer">
    <div class="container">
        <div class="row">
        </div>
    </div>

    <ul class="icons">
    </ul>

    <div class="copyright">
        <ul class="menu">
            
                <li>Design: <a href="https://html5up.net">HTML5 UP</a>
                <li><a href="https://github.com/half-duplex/hugo-arcana">Theme</a>
        </ul>
    </div>
</div>
</div><script src="/HumDial-Challenge/js/jquery.min.js"></script>
<script src="/HumDial-Challenge/js/jquery.dropotron.min.js"></script>
<script src="/HumDial-Challenge/js/browser.min.js"></script>
<script src="/HumDial-Challenge/js/breakpoints.min.js"></script>
<script src="/HumDial-Challenge/js/util.js"></script>
<script src="/HumDial-Challenge/js/main.js"></script>
</body>
</html>
